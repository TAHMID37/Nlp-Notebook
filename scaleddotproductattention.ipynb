{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d951e26-7f44-4415-ba5e-24b01c8c3869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Words:\n",
      "['tired', 'too', 'was', 'it', 'because', 'cross', \"didn't\", 'animal', 'The', 'the', 'street']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q.float(), k.transpose(1, 2).float()) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v.float())\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "input_string = \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "# Tokenize the input string into words\n",
    "words = input_string.split()\n",
    "\n",
    "# Convert words to indices or embeddings using appropriate tokenization and embedding methods\n",
    "# In this example, we'll use a simple word-to-index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "# Convert the input string to a sequence of indices\n",
    "input_indices = [word_to_index[word] for word in words]\n",
    "\n",
    "# Create input tensors\n",
    "q = torch.tensor(input_indices).unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_len, 1)\n",
    "k = q.clone()  # Same as q for self-attention\n",
    "v = q.clone()  # Same as q for self-attention\n",
    "\n",
    "# Create attention mask\n",
    "mask = torch.ones_like(q)  # Shape: (1, seq_len, 1)\n",
    "\n",
    "# Initialize the ScaledDotProductAttention module\n",
    "attention = ScaledDotProductAttention(temperature=1.0)\n",
    "\n",
    "# Perform the attention calculation\n",
    "output, attn = attention(q, k, v, mask=mask)\n",
    "\n",
    "# Find the index of the word \"it\" in the input string\n",
    "it_index = word_to_index[\"it\"]\n",
    "\n",
    "# Get the attention matrix for the word \"it\"\n",
    "attention_matrix = attn[0, it_index, :]\n",
    "\n",
    "# Sort the words based on attention weights in descending order\n",
    "sorted_indices = torch.argsort(attention_matrix, descending=True)\n",
    "sorted_words = [words[idx] for idx in sorted_indices]\n",
    "\n",
    "print(\"Sorted Words:\")\n",
    "print(sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa32ea9-ae8f-4a2c-a3dc-054f6a9ab4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
