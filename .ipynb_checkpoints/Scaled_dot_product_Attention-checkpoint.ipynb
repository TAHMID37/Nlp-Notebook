{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66ddb19f-c7c6-4d64-89d1-fea0d607cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee50bfb-d697-4200-9a43-1d230a49e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q.float(), k.transpose(1, 2).float()) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v.float())\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0f30ee-b1ea-4ac2-99a5-16a8b9c2c094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered Words:\n",
      "['The', 'animal', \"didn't\", 'cross', 'the', 'street', 'because', 'it', 'was', 'too', 'tired']\n"
     ]
    }
   ],
   "source": [
    "input_string = \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "# Tokenize the input string into words\n",
    "words = input_string.split()\n",
    "\n",
    "# Convert words to indices or embeddings using appropriate tokenization and embedding methods\n",
    "# In this example, we'll use a simple word-to-index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "# Convert the input string to a sequence of indices\n",
    "input_indices = [word_to_index[word] for word in words]\n",
    "\n",
    "# Create input tensors\n",
    "q = torch.tensor(input_indices).unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_len, 1)\n",
    "k = q.clone()  # Same as q for self-attention\n",
    "v = q.clone()  # Same as q for self-attention\n",
    "\n",
    "# Create attention mask\n",
    "mask = torch.ones_like(q)  # Shape: (1, seq_len, 1)\n",
    "\n",
    "# Initialize the ScaledDotProductAttention module\n",
    "attention = ScaledDotProductAttention(temperature=1.0)\n",
    "\n",
    "# Perform the attention calculation\n",
    "output, attn = attention(q, k, v, mask=mask)\n",
    "\n",
    "# Find the index of the word \"it\" in the input string\n",
    "it_index = word_to_index[\"it\"]\n",
    "\n",
    "# Get the attention matrix for the word \"it\"\n",
    "attention_matrix = attn[0, :, it_index]\n",
    "\n",
    "# Order the words according to the attention matrix in descending order\n",
    "ordered_indices = torch.argsort(attention_matrix, descending=True)\n",
    "ordered_words = [words[index] for index in ordered_indices]\n",
    "print(\"Ordered Words:\")\n",
    "print(ordered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "419279e7-2927-4f10-83ed-771e9fd793cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered Words:\n",
      "['tired', 'too', 'was', 'it', 'because', 'street', 'cross', 'animal', 'The', \"didn't\", 'the']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q.float(), k.transpose(1, 2).float()) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v.float())\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "input_string = \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "# Tokenize the input string into words\n",
    "words = input_string.split()\n",
    "\n",
    "# Convert words to indices or embeddings using appropriate tokenization and embedding methods\n",
    "# In this example, we'll use a simple word-to-index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "# Convert the input string to a sequence of indices\n",
    "input_indices = [word_to_index[word] for word in words]\n",
    "\n",
    "# Create input tensors\n",
    "q = torch.tensor(input_indices).unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_len, 1)\n",
    "k = q.clone()  # Same as q for self-attention\n",
    "v = q.clone()  # Same as q for self-attention\n",
    "\n",
    "# Create attention mask\n",
    "mask = torch.ones_like(q)  # Shape: (1, seq_len, 1)\n",
    "\n",
    "# Initialize the ScaledDotProductAttention module\n",
    "attention = ScaledDotProductAttention(temperature=1.0)\n",
    "\n",
    "# Perform the attention calculation\n",
    "output, attn = attention(q, k, v, mask=mask)\n",
    "\n",
    "# Find the index of the word \"it\" in the input string\n",
    "it_index = word_to_index[\"it\"]\n",
    "\n",
    "# Get the attention matrix for the word \"it\"\n",
    "attention_matrix = attn[0, it_index, :]\n",
    "\n",
    "# Order the words according to the attention matrix in descending order\n",
    "ordered_indices = torch.argsort(attention_matrix, descending=True)\n",
    "ordered_words = [words[index] for index in ordered_indices]\n",
    "\n",
    "print(\"Ordered Words:\")\n",
    "print(ordered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4bf0970-ca69-4fb9-8043-1aeac9cbd6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Words:\n",
      "['tired', 'was', 'it', 'because', 'street', 'cross', \"didn't\", 'animal', 'The', 'the', 'too']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q.float(), k.transpose(1, 2).float()) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v.float())\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "input_string = \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "# Tokenize the input string into words\n",
    "words = input_string.split()\n",
    "\n",
    "# Convert words to indices or embeddings using appropriate tokenization and embedding methods\n",
    "# In this example, we'll use a simple word-to-index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "# Convert the input string to a sequence of indices\n",
    "input_indices = [word_to_index[word] for word in words]\n",
    "\n",
    "# Create input tensors\n",
    "q = torch.tensor(input_indices).unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_len, 1)\n",
    "k = q.clone()  # Same as q for self-attention\n",
    "v = q.clone()  # Same as q for self-attention\n",
    "\n",
    "# Create attention mask\n",
    "mask = torch.ones_like(q)  # Shape: (1, seq_len, 1)\n",
    "\n",
    "# Initialize the ScaledDotProductAttention module\n",
    "attention = ScaledDotProductAttention(temperature=1.0)\n",
    "\n",
    "# Perform the attention calculation\n",
    "output, attn = attention(q, k, v, mask=mask)\n",
    "\n",
    "# Find the index of the word \"it\" in the input string\n",
    "it_index = word_to_index[\"it\"]\n",
    "\n",
    "# Get the attention matrix for the word \"it\"\n",
    "attention_matrix = attn[0, it_index, :]\n",
    "\n",
    "# Find the indices of the words in descending order of attention weights\n",
    "sorted_indices = torch.argsort(attention_matrix, descending=True)\n",
    "\n",
    "\n",
    "# Get the words in the sorted order\n",
    "sorted_words = [words[idx] for idx in sorted_indices]\n",
    "\n",
    "print(\"Sorted Words:\")\n",
    "print(sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87734c45-7bc9-44f3-ae23-d2b8020c2240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Words:\n",
      "['tired', 'too', 'was', 'because', 'street', 'the', 'cross', \"didn't\", 'animal', 'The', 'it']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q.float(), k.transpose(1, 2).float()) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v.float())\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "input_string = \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "# Tokenize the input string into words\n",
    "words = input_string.split()\n",
    "\n",
    "# Convert words to indices or embeddings using appropriate tokenization and embedding methods\n",
    "# In this example, we'll use a simple word-to-index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "# Convert the input string to a sequence of indices\n",
    "input_indices = [word_to_index[word] for word in words]\n",
    "\n",
    "# Create input tensors\n",
    "q = torch.tensor(input_indices).unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_len, 1)\n",
    "k = q.clone()  # Same as q for self-attention\n",
    "v = q.clone()  # Same as q for self-attention\n",
    "\n",
    "# Create attention mask\n",
    "mask = torch.ones_like(q)  # Shape: (1, seq_len, 1)\n",
    "\n",
    "# Initialize the ScaledDotProductAttention module\n",
    "attention = ScaledDotProductAttention(temperature=1.0)\n",
    "\n",
    "# Perform the attention calculation\n",
    "output, attn = attention(q, k, v, mask=mask)\n",
    "\n",
    "# Find the index of the word \"it\" in the input string\n",
    "it_index = word_to_index[\"it\"]\n",
    "\n",
    "# Get the attention matrix for the word \"it\"\n",
    "attention_matrix = attn[0, it_index, :]\n",
    "\n",
    "# Find the indices of the words in descending order of attention weights\n",
    "sorted_indices = torch.argsort(attention_matrix, descending=True)\n",
    "\n",
    "# Get the words in the sorted order\n",
    "sorted_words = [words[idx] for idx in sorted_indices if words[idx] != \"it\"] + [\"it\"]\n",
    "\n",
    "print(\"Sorted Words:\")\n",
    "print(sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "127041ef-b2da-4ada-83da-eacba3772484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Words:\n",
      "['tired', 'too', 'was', 'the', 'cross', \"didn't\", 'animal', 'The', 'street', 'because', 'it']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q.float(), k.transpose(1, 2).float()) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v.float())\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "input_string = \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "# Tokenize the input string into words\n",
    "words = input_string.split()\n",
    "\n",
    "# Convert words to indices or embeddings using appropriate tokenization and embedding methods\n",
    "# In this example, we'll use a simple word-to-index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "# Convert the input string to a sequence of indices\n",
    "input_indices = [word_to_index[word] for word in words]\n",
    "\n",
    "# Create input tensors\n",
    "q = torch.tensor(input_indices).unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_len, 1)\n",
    "k = q.clone()  # Same as q for self-attention\n",
    "v = q.clone()  # Same as q for self-attention\n",
    "\n",
    "# Create attention mask\n",
    "mask = torch.ones_like(q)  # Shape: (1, seq_len, 1)\n",
    "\n",
    "# Initialize the ScaledDotProductAttention module\n",
    "attention = ScaledDotProductAttention(temperature=1.0)\n",
    "\n",
    "# Perform the attention calculation\n",
    "output, attn = attention(q, k, v, mask=mask)\n",
    "\n",
    "# Find the index of the word \"it\" in the input string\n",
    "it_index = word_to_index[\"it\"]\n",
    "\n",
    "# Get the attention matrix for the word \"it\"\n",
    "attention_matrix = attn[0, it_index, :]\n",
    "\n",
    "# Sort the words based on attention weights in descending order\n",
    "sorted_indices = torch.argsort(attention_matrix, descending=True)\n",
    "sorted_words = [words[idx] for idx in sorted_indices]\n",
    "\n",
    "print(\"Sorted Words:\")\n",
    "print(sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3de248-1b26-4038-9963-55094d46f4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c43eb9-99d1-4fa1-9ca9-46441a23c7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
